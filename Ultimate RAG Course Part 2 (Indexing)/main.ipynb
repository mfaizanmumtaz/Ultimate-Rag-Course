{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4,os\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "blog_docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50)\n",
    "\n",
    "text = text_splitter.split_documents(blog_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "num_tokens_from_string(text[10].page_content, \"cl100k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is overfitting?\"\n",
    "\n",
    "documents = [\"\"\"Overfitting happens when a machine learning model learns the training data too well, including noise or random fluctuations that are specific to the training set but don't generalize well to new, unseen data.\"\"\",\n",
    "             \n",
    "             \"To reduce overfitting, we should use some regularization techniques and perform cross-validation to find the best hyperparameters. Additionally, reducing overfitting is not just about randomly trying different methods; it requires a thoughtful approach. Developing an understanding of when and how to use different techniques will save time and effort and improve your expertise.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_cohere import CohereEmbeddings\n",
    "embeddings = CohereEmbeddings(model=\"embed-english-light-v3.0\",cohere_api_key=\"Enter api key here.\")\n",
    "query_result = embeddings.embed_query(question)\n",
    "documents_result = embeddings.embed_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "score_dict = {}\n",
    "for idx, doc_vec in enumerate(documents_result):\n",
    "    score_dict[documents[idx]] = cosine_similarity(query_result, doc_vec)\n",
    "\n",
    "print(\"Question Cosine Similarity with different vectors:\\n\")\n",
    "print(f\"Question: {question}\\n\")\n",
    "\n",
    "for doc, score in score_dict.items():\n",
    "    print(f\"{doc[:40]}...: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_1 = \"\"\"I love Python.\"\"\"\n",
    "\n",
    "in_2 = \"\"\"I love JavaScript.\"\"\"\n",
    "\n",
    "in_3 = \"\"\"I love Palestine.\"\"\" \n",
    "\n",
    "in_4 = \"\"\"Islam is true religious.\"\"\" \n",
    "\n",
    "in_5 = \"\"\"Islam is true and world largest religious.\"\"\" \n",
    "\n",
    "question = \"\"\"Which religious is true.\"\"\"\n",
    "\n",
    "\n",
    "input_text_lst_sim = [in_1, in_2, in_3, in_4,in_5,question]\n",
    "\n",
    "_embeddings = []\n",
    "for chunk in input_text_lst_sim:\n",
    "    _embeddings.append(embeddings.embed_query(chunk))\n",
    "\n",
    "print(_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract the first two dimensions for plotting (assuming embeddings have more than 2 dimensions)\n",
    "embeddings_2d = [embedding[:2] for embedding in _embeddings]\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(6, 4))\n",
    "for i, text in enumerate(input_text_lst_sim):\n",
    "  x, y = embeddings_2d[i]\n",
    "  plt.scatter(x, y)\n",
    "  plt.annotate(text, (x, y), textcoords=\"offset points\", xytext=(0, -10),fontsize=8)\n",
    "  \n",
    "plt.title(\"2D Visualization of Embeddings\")\n",
    "plt.xlabel(\"Dimension 1\")\n",
    "plt.ylabel(\"Dimension 2\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "db = Chroma.from_texts(persist_directory=\"./chroma_db8\",embedding=embeddings,texts=text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
